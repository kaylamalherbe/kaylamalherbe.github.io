# A2 Notes

## Adam opt
The Adam optimizer is a popular optimization algorithm used in machine learning for stochastic gradient descent (SGD)-based optimization. 
It stands for Adaptive Moment Estimation and combines the best parts of two other optimization algorithms, AdaGrad and RMSProp.

Adam compared to alternatvies 

<img width="660" height="641" alt="image" src="https://github.com/user-attachments/assets/b1cc9161-5e73-4e2f-9513-6c4ef85435e0" />

## GDS
Gradient descent
<img width="3116" height="2808" alt="image" src="https://github.com/user-attachments/assets/40340250-0401-494b-9f64-32c174bb6349" />


## VAE
<img width="826" height="374" alt="image" src="https://github.com/user-attachments/assets/eedc9159-f7bc-4a48-8654-b03ec3e84924" />

Variational auto encoder consists of an ecoder a decoder and a loss function. The encoder is a neural net, and the decoder is another neural net


## UNET

# RESTNET-18
